# Counter AI. Stackoverflow Answers GPT Detection

An API for identifying AI-generated responses to the given answers

## Background

ChatGPT is being an ongoing sensation and source of anxiety and heated debates for several months now. 
One of the main concerns is if AI can mimic humans too well, which can have different negative consequences. 
Among such consequences are plausible but incorrect answers that AI can give to people asking for help on the web.

Soon after ChatGPT became a thing, StackOverflow declared they would stop accepting answers from it. However, they didn't disclose how they were identifying them.
I thought it would be a great idea to research and implement my own classifier.

### How it differs from other AI-detection systems

The interesting part of my project is that it identifies AI-generated content given such context as human-generated question. 
This brings interesting opportunities for using different metrics that connect the question and the answer.

## Use Cases

Such an API can be used by automated premoderation systems that check if an answer was generated by a human or AI

## Problem statement
The objective of this project is to develop a production-ready classifier API that, given a pair of texts (question and answer), identifies if the answer was generated by the GPT-3 language model or by a human.

## Data collection
The training data was collected using public API from StackOverflow, as well as public API from OpenAI that exposes GPT-3 language model.
This model is used behind the well-known ChatGPT AI chatbot

For the whole data collection and preparation flow please refer to the [data_collect.ipynb](data_collect.ipynb) notebook

### StackOverflow data
* To collect questions and human-generated answers I was using StackExchange API 2.2
  * Please refer to these scripts: 
    * [download_questions.py](download_questions.py)
    * [download_answers.py](download_answers.py)
* Only Stackoverflow site was used
* The most popular 10000 questions that had an accepted answer were fetched
* The accepted answer for every question was fetched as well

### OpenAI data 
* To collect the AI-generated answers I was using public OpenAI API with `text-davinci-003` language model.
* The stackoverflow question concatenated with its title was used as an input

## EDA and model training
For the complete flow please refer to the [eda_and_train.ipynb](eda_and_train.ipynb) notebook

I used the following approaches while training the model:
* Feature-based approach
* Distribution-based approach
* Combination of the previous two

### Feature-based Analysis
The intuition behind this approach is that GPT tends to use specific patterns in how it creates an answer based on a human-generated answer. For example, it seems to reuse whole sentences from the question 
The idea here is to engineer several numerical features and train a linear classifier on them.

Main steps:
* Tokenizing the answers and question into separate words
* Stemming the tokens. This means reducing different forms of a word (i.e. plural and singular) to a singular token (stem)
* Creating ngrams (all different sequences of words of length `n`, where `n` varies from 2 to max)
* Creating numerical features
* Training Logistic Regression Classifier

#### Feature Engineering
##### Creativity
* Assumption: 
  * AI tends to use less original words in the answer comparing to the question than humans
* Features:
  * `creativity`: number of all unique words in the answer that are not present in the question, divided by the size of the answer

##### Vocabulary
* Assumption:
  * AI tends to use overall less unique words in the answer than humans
* Features:
  * `vocabulary_size`: number of unique words in the answer divided by the size of the tokenized answer

##### Stealing
* Assumption:
  * AI tends to reuse ("steal") in the answer more words and `ngrams` (sequences of words of length `n`) from the question than humans
  * Opposite to creativity, stealing shows how many words sequences were "stolen" from the question and taken into the answer. 
* Features
  * `stealing_strength`: maximum length (number of words) of a stolen ngram in the answer
  * `stealing_frequency`: total number of stolen ngrams divided by number of unique words in the answer

##### Answer length
* Assumption:
  * Even though the answer length is limited for humans and AI, the latter tends to create answers closer to some particular length than humans
* Features:
  * `answer_length`: answer length as number of characters

##### Sentence length
* Assumption:
  * AI tends to generate sentence by a specific pattern, different from humans'
* Features
  * `sentence_length_mean`: mean length of sentences in the answer
  * `sentence_length_std`: standard deviation length of sentences in the answer

#### Results

![features_input.png](features_input.png)

* Despite the results of correlation matrix and histograms, the following features were selected after trying cross-validation score:
  * `creativity`
  * `vocabulary_size` 
  * `stealing_strength`
  * `answer_length`
  * `sentence_length_mean`
* After fine-tuning, the F1 score on the test data was:
  * 0.835

### Distribution-based Approach
The intuition behind this approach is that GPT has a specific vocabulary and some terms (single words and ngrams) tend to appear in its answers more frequently than in humans'.
Main steps of this approach are:
* Tokenizing and stemming the answers
* Vectorizing the answers using `CountVectorizer`. 
* Applying Naive Bayes classifier (`ComplementNB`) to the vectors

#### Results
* The best `ngram_range` is (1, 3). That means that up to 3-words sequences have significance for the word frequency analysis
* The F1 score on the test data was:
  * 0.879


### Combined approach
Both previous approached showed already good scores (0.835 and 0.879)
However they are dealing with different aspects of data, so it was worth trying to combine them into 2-layered pipeline

Main steps:
* Stemming and vectorizing the answers
* Applying Naive Bayes to the vectors to predict probability
* Add this probability as another numerical feature to already created and selected features
* Train Logistic Regression on the new set of numerical features

#### Results
* The F1 score on the test data was:
  * 0.904

This is my capstone project for the machine learning bootcamp:
https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp
